{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c044f5",
   "metadata": {},
   "source": [
    "# Project: Spam Detection\n",
    "\n",
    "- Date: Not marked - August 5 2025 \n",
    "\n",
    "- Data: The data used in this project come from SpamAssasin Public Corpus. we can find them by this link: https://spamassassin.apache.org/old/publiccorpus/ \n",
    "\n",
    "- Description: This project aims to predict, regarding an email, if it's a spam or a ham. Then I'll try to give message that I recieved and let the model predict if it is spam or ham."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c270349",
   "metadata": {},
   "source": [
    "## Downloading and Cleaning of data\n",
    "\n",
    "Firstly we will try to find the data and transform it into a type we can use.\n",
    "The following code box was made by Chatgptand I made some modifications because I don't know how to download the data and this is not the main goal of that project.\n",
    "But I understand it more now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e01e3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christian/ProjetsPerso/IA/MachineLearning/Spam_Detection/data/20030228_easy_ham_2\n",
      "Downloaded: 6552 messages (2399 spam, 4153 ham)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "def load_messages(root_folder=\"publiccorpus\"):\n",
    "    paths = glob.glob(f\"{root_folder}/**/*\", recursive=True)\n",
    "    texts, labels = [], []\n",
    "\n",
    "    print(paths[0])\n",
    "    for path in paths:\n",
    "        # Ignorer les dossiers\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                msg = BytesParser(policy=policy.default).parse(f)\n",
    "        except IsADirectoryError:\n",
    "            continue\n",
    "\n",
    "        # Extraction du body (plain ou html)\n",
    "        body = msg.get_body(preferencelist=('plain', 'html'))\n",
    "        #if not body: print(\"NOTHINGGGGG\", msg.get_content)\n",
    "        raw_text = \"\"\n",
    "        if body:\n",
    "            try:\n",
    "                # méthode standard (peut lever LookupError)\n",
    "                raw_text = body.get_content()\n",
    "            except LookupError:\n",
    "                # fallback : décoder le payload manuellement\n",
    "                payload = body.get_payload(decode=True) or b\"\"\n",
    "                try:\n",
    "                    raw_text = payload.decode('utf-8', errors='replace')\n",
    "                except (UnicodeDecodeError, LookupError):\n",
    "                    raw_text = payload.decode('latin-1', errors='replace')\n",
    "\n",
    "        texts.append(raw_text)\n",
    "        labels.append(0 if \"ham\" in path.lower() else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    texts, labels = load_messages(\"/home/christian/ProjetsPerso/IA/MachineLearning/Spam_Detection/data\")\n",
    "    print(f\"Downloaded: {len(texts)} messages ({sum(labels)} spam, {len(labels)-sum(labels)} ham)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "10990833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quoting Niall O Broin <niall@linux.ie>:\\n\\n> I\\'m installing warm standby disks on a number of boxes. These disks will be\\n> the same size (sometimes bigger) than the main disk. The idea is that every\\n> night I\\'ll rsync the partitions on the main disk to the standby disk so\\n> that\\n> in the case of disaster, the first port of call, before the tapes, is the\\n> standby disk. (We did consider running Linux md RAID on the disks but RAID\\n> gives you no protection against slips of the finger)\\n\\nDo I get beaten round the head for saying \"floppy\"?\\nAssuming the machines are networked, let each one send a copy of its kernel to\\nthe others.  If the drives are open-the-box-and-switch-cables, then you can\\nstart dd\\'ing a floppy before you start.  If the drives are in drawers, then this\\nmight slow you down by all of 60 seconds.\\n\\nAlternatively, you could use netboot.  No, I\\'m serious.  Set the boot sequence\\nto first hard disk then network.  Do NOT make any partition on the standby\\nactive.  Have a look at the etherboot package.  One of the things it contains is\\na pascal-ish language for writing boot menus.  You can write a one-liner that\\nbasically says \"boot /dev/hda1\" (or whatever, there\\'s example code).  IIRC, the\\nresulting \"bootable image\" is a whopping 4K.  The downside is you\\'ll need a\\nbootp and tftp server somewhere....\\n\\nhth,\\nRonan.\\n\\n\\n-- \\nIrish Linux Users\\' Group: ilug@linux.ie\\nhttp://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\\nList maintainer: listmaster@linux.ie\\n\\n\\n']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "#Display of some text\n",
    "print(texts[0:1])\n",
    "print(type(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0141e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Cleaning part:\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3923ef",
   "metadata": {},
   "source": [
    "In this phase, we will do what we call preprocessing the text.\n",
    "\n",
    "It consists into tranform all characters in the text in a lowercase, then delete all special characters(:,@,!,?,...), all email adresses and keep only letters and numbers. Then we transform the text in different tokens and delete the stopwords(very used word with no real meaning or importance like \"the\",\"of\", \"is\", \"a\", \"but\",...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "61ad01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_nltk(text):\n",
    "    tokens=[]# For the text's tokens\n",
    "    stop_words = set(stopwords.words('english')) # all the stopword in english(eg:\"the\",\"is\",\"of\"...) because not very useful for our need\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    #Conversion en minuscule\n",
    "    text=text.lower()\n",
    "    \n",
    "    #Suppression of pontuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    #Tokenization\n",
    "    tokens=word_tokenize(text)\n",
    "\n",
    "    #Suppresion of stopword\n",
    "    tokens=[w for w in tokens if w not in stop_words]\n",
    "\n",
    "    #Stemming (reduction of the number of token by gathering different variant of the same word or token)\n",
    "    tokens=[stemmer.stem(w) for w in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ddc2c",
   "metadata": {},
   "source": [
    "We will transform the text in a numeric vetor that could be used in our models. This transformation give a score we call TD-IDF which gave more importance to less used words in a text.\n",
    "\n",
    "That is very smart and interesting, that's means that to make a difference we can not focus on the most used words in the text but on the less used words in the text. Indeed, generally the most used words in a text will be also very used in another text so focusing on that will not help us to make find differences between the texts, so we focus on the less used one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1df3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(preprocessor=preprocess_text_nltk,\n",
    "                           tokenizer=lambda txt: txt.split())\n",
    "X=vectorizer.fit_transform(texts)\n",
    "y=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d38f664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (6552, 46416)\n",
      "['0' '00' '000' ... 'zzzzason' 'zzzzcc' 'zzzzteana']\n",
      "Matrix:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Vocab size: 46416\n",
      "Some features:\n",
      " ['quot', 'niall', 'broin', 'instal', 'warm', 'standbi', 'disk', 'number', 'box', 'size', 'sometim', 'bigger', 'main', 'idea', 'everi', 'night', 'rsync', 'partit', 'case', 'disast']\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", X.shape)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"Matrix:\\n\",X.toarray())\n",
    "print(\"Vocab size:\", len(vectorizer.vocabulary_))\n",
    "print(\"Some features:\\n\", list(vectorizer.vocabulary_.keys())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e79eaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> hello moimem test email offer insid\n",
      "-> urgent 1000 click\n",
      "-> bonjour ceci est un email l gitim\n",
      "Shape: (3, 15)\n",
      "Vocab: ['1000' 'bonjour' 'ceci' 'click' 'email' 'est' 'gitim' 'hello' 'insid' 'l'\n",
      " 'moimem' 'offer' 'test' 'un' 'urgent']\n",
      "Matrix:\n",
      " [[0.         0.         0.         0.         0.32200242 0.\n",
      "  0.         0.42339448 0.42339448 0.         0.42339448 0.42339448\n",
      "  0.42339448 0.         0.        ]\n",
      " [0.57735027 0.         0.         0.57735027 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.57735027]\n",
      " [0.         0.38988801 0.38988801 0.         0.29651988 0.38988801\n",
      "  0.38988801 0.         0.         0.38988801 0.         0.\n",
      "  0.         0.38988801 0.        ]]\n",
      "['1000' 'bonjour' 'ceci' 'click' 'email' 'est' 'gitim' 'hello' 'insid' 'l'\n",
      " 'moimem' 'offer' 'test' 'un' 'urgent']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Little test to know how it is work\n",
    "texts_raw = [\n",
    "    \"Hello @moimemeéé, this is a test email! Offer inside.\",\n",
    "    \"URGENT: You have won $1000. Click here!!!\",\n",
    "    \"Bonjour, ceci est un email légitime.\"\n",
    "]\n",
    "\n",
    "vectorizer2 = TfidfVectorizer(\n",
    "    preprocessor=preprocess_text_nltk,\n",
    "    tokenizer=lambda txt: txt.split(),\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "## 1 Debug preprocess\n",
    "for txt in texts_raw:\n",
    "    print(\"->\", preprocess_text_nltk(txt))\n",
    "\n",
    "## 2 Fit transform\n",
    "X_test2 = vectorizer2.fit_transform(texts_raw)\n",
    "print(\"Shape:\", X_test2.shape)\n",
    "print(\"Vocab:\", vectorizer2.get_feature_names_out())\n",
    "print(\"Matrix:\\n\", X_test2.toarray())\n",
    "print(vectorizer2.get_feature_names_out())\n",
    "print(type(texts_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f5274",
   "metadata": {},
   "source": [
    "## Building of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "76b473a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d26a76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression :\n",
      "\t Accuracy score:  0.9557589626239512\n",
      "\t Precision score:  0.9709821428571429\n",
      "\t Recall score:  0.90625\n",
      "\t F1 score :  0.9375\n",
      "K-Nearest Neighbor :\n",
      "\t Accuracy score:  0.5316552250190694\n",
      "\t Precision score:  0.43703007518796994\n",
      "\t Recall score:  0.96875\n",
      "\t F1 score :  0.6023316062176166\n",
      "Random Forest :\n",
      "\t Accuracy score:  0.9748283752860412\n",
      "\t Precision score:  0.976545842217484\n",
      "\t Recall score:  0.9541666666666667\n",
      "\t F1 score :  0.9652265542676501\n",
      "Gradient Bosting :\n",
      "\t Accuracy score:  0.9687261632341724\n",
      "\t Precision score:  0.9720430107526882\n",
      "\t Recall score:  0.9416666666666667\n",
      "\t F1 score :  0.9566137566137566\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.8,\n",
    "                                               random_state=42,#to keep the same result\n",
    "                                               stratify=y)\n",
    "\n",
    "#Building of the models and fitting\n",
    "reg_log =LogisticRegression().fit(X_train,y_train)\n",
    "reg_for =RandomForestClassifier().fit(X_train,y_train)\n",
    "reg_grad=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "reg_nn  =KNeighborsClassifier(n_neighbors=5).fit(X_train,y_train)\n",
    "\n",
    "#Prediction\n",
    "y_pred_log =reg_log.predict(X_test) \n",
    "y_pred_for =reg_for.predict(X_test) \n",
    "y_pred_grad=reg_grad.predict(X_test)\n",
    "y_pred_nn  =reg_nn.predict(X_test)\n",
    "\n",
    "#Evaluation\n",
    "def printMetrics(method_title, y_predict, y_true):\n",
    "    print(method_title) \n",
    "    print(\"\\t Accuracy score: \", accuracy_score(y_true, y_predict))\n",
    "    print(\"\\t Precision score: \", precision_score(y_true, y_predict))\n",
    "    print(\"\\t Recall score: \", recall_score(y_true, y_predict))\n",
    "    print(\"\\t F1 score : \", f1_score(y_true, y_predict))\n",
    "\n",
    "\n",
    "printMetrics(\"Logistic Regression :\",y_pred_log, y_test)\n",
    "printMetrics(\"K-Nearest Neighbor :\",y_pred_nn, y_test)\n",
    "printMetrics(\"Random Forest :\",y_pred_for, y_test)\n",
    "printMetrics(\"Gradient Bosting :\",y_pred_grad, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7c45f",
   "metadata": {},
   "source": [
    "We can notice that the logistic regression, the random forest and the Gradient bosting models are very good because their score are very high. However, the K-neirest neighbor model is not very appropriate for this task even if he predict correctly the good tweet and have the best score for that task ( highest score for the recall score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba45826",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Here we will give to our models some real spam and ham messages recieved and see what will be the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c133cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christian/ProjetsPerso/Artificial_Intelligence/MachineLearning/Spam_Detection/test/test\n",
      "no body\n",
      "no body\n",
      "no body\n",
      "no body\n",
      "[0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "\n",
    "def load_messages(root_folder=\"publiccorpus\"):\n",
    "    paths = glob.glob(f\"{root_folder}/**/*\", recursive=True)\n",
    "    texts, labels = [], []\n",
    "\n",
    "    print(paths[0])\n",
    "    for path in paths:\n",
    "        # Ignorer les dossiers\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                msg = BytesParser(policy=policy.default).parse(f)\n",
    "        except IsADirectoryError:\n",
    "            continue\n",
    "\n",
    "        # Extraction du body (plain ou html)\n",
    "        body = msg.get_body(preferencelist=('plain', 'html'))\n",
    "        raw_text = \"\"\n",
    "        if body:\n",
    "            print(\"body\")\n",
    "            try:\n",
    "                # méthode standard (peut lever LookupError)\n",
    "                raw_text = body.get_content()\n",
    "            except LookupError:\n",
    "                # fallback : décoder le payload manuellement\n",
    "                payload = body.get_payload(decode=True) or b\"\"\n",
    "                try:\n",
    "                    raw_text = payload.decode('utf-8', errors='replace')\n",
    "                except (UnicodeDecodeError, LookupError):\n",
    "                    raw_text = payload.decode('latin-1', errors='replace')\n",
    "        \n",
    "        if not body and not msg.is_multipart():\n",
    "            print(\"no body\")\n",
    "            try:\n",
    "                raw_text=msg.get_content().strip()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "        texts.append(raw_text)\n",
    "        labels.append(0 if \"ham\" in path.lower() else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "text_test,labels_test =load_messages(\"/home/christian/ProjetsPerso/Artificial_Intelligence/MachineLearning/Spam_Detection/test/\")\n",
    "\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "313a3f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Sir,\\n\\nI am writing this email on behalf of the whole group to thank you for the time you gave us last Saturday.\\n\\n\\nThis initial interview was extremely useful, both in terms of our choice of specialization in our second year and the important, up-to-date information you provided about the data scientist profession. It also helped us gain a clearer picture of our entire journey at Ensimag to achieve our goals.\\n\\n\\nWe hope to see you again one day. Thank you, and see you soon.\\n\\n\\nBest regards.', 'Hello,\\n\\nPlease send me the documents listed below as a matter of urgency. Some documents are attached and need to be completed.\\n\\nPlease send them by email to prepa-ginp@lycee-blaisepascal.com or to the office by Monday, September 20 at the latest:\\n\\n    A copy of the civil status document for the student and their parents (ID card or passport)\\n    A certificate of nationality for the student,\\n    The medical file completed by a doctor,\\n    A copy of the vaccination record,\\n    The delegation of responsibility if you live far from Abidjan and your child is boarding,\\n    The care protocol attached, \\n\\n\\nThank you.', 'Continental���s Automotive group sector is on an exciting journey to become AUMOVIO and you have the opportunity to be part of it from early on! \\n\\nYour application for an Automotive position, was now moved to our new AUMOVIO Applicant Tracking System to continue our selection process. \\n\\n \\n\\nAlso at AUMOVIO, safeguarding the security and privacy of your personal data is of highest importance to us, and we strive to adhere to relevant laws and industry standards.  \\n\\nTherefore, we would like to inform you about our AUMOVIO Data Privacy Policy and kindly ask you to indicate your acknowledgment of our privacy policy here.  \\n\\n \\n\\nReview policy\\n\\n \\n\\nPlease note, by checking the third box, you agree to be part of the AUMOVIO talent pool for future career opportunities and your application will be stored for the purpose of contacting you with future job opportunities by e-mail or phone. For this purpose, we will forward your application within AUMOVIO Group (�� 15 AktG). You can withdraw your consent at any time by unregistering from the talent pool, or by sending an email at privacy@aumovio.com, or by sending a notice though the contact channels stated in the imprint.  \\n\\n \\n\\nIf you do not agree within 30 days, your profile (name, last name and e-mail address) will be deleted automatically. You can also view, update or delete your profile at any time.', \"Finished your presentation? Use Magic Transform to turn it into a summary document. Go to Magic Transform in the top menu bar of your file. We'll summarize your presentation in seconds.\"]\n"
     ]
    }
   ],
   "source": [
    "print(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d2f1ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5923b893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression :\n",
      "\t Accuracy score:  0.75\n",
      "\t Precision score:  1.0\n",
      "\t Recall score:  0.5\n",
      "\t F1 score :  0.6666666666666666\n",
      "\t True value: 0 Prediction: 0\n",
      "\t True value: 0 Prediction: 0\n",
      "\t True value: 1 Prediction: 1\n",
      "\t True value: 1 Prediction: 0\n",
      "K-Nearest Neighbor :\n",
      "\t Accuracy score:  0.5\n",
      "\t Precision score:  0.5\n",
      "\t Recall score:  1.0\n",
      "\t F1 score :  0.6666666666666666\n",
      "\t True value: 0 Prediction: 1\n",
      "\t True value: 0 Prediction: 1\n",
      "\t True value: 1 Prediction: 1\n",
      "\t True value: 1 Prediction: 1\n",
      "Random Forest :\n",
      "\t Accuracy score:  0.75\n",
      "\t Precision score:  1.0\n",
      "\t Recall score:  0.5\n",
      "\t F1 score :  0.6666666666666666\n",
      "\t True value: 0 Prediction: 0\n",
      "\t True value: 0 Prediction: 0\n",
      "\t True value: 1 Prediction: 1\n",
      "\t True value: 1 Prediction: 0\n",
      "Gradient Bosting :\n",
      "\t Accuracy score:  0.75\n",
      "\t Precision score:  1.0\n",
      "\t Recall score:  0.5\n",
      "\t F1 score :  0.6666666666666666\n",
      "\t True value: 0 Prediction: 0\n",
      "\t True value: 0 Prediction: 0\n",
      "\t True value: 1 Prediction: 1\n",
      "\t True value: 1 Prediction: 0\n"
     ]
    }
   ],
   "source": [
    "X_test=vectorizer.transform(text_test)\n",
    "y_test=labels_test\n",
    "\n",
    "#Predictions\n",
    "y_pred_log =reg_log.predict(X_test) \n",
    "y_pred_for =reg_for.predict(X_test) \n",
    "y_pred_grad=reg_grad.predict(X_test)\n",
    "y_pred_nn  =reg_nn.predict(X_test)\n",
    "\n",
    "#Evaluations\n",
    "def printMetrics(method_title, y_predict, y_true):\n",
    "    print(method_title) \n",
    "    print(\"\\t Accuracy score: \", accuracy_score(y_true, y_predict))\n",
    "    print(\"\\t Precision score: \", precision_score(y_true, y_predict))\n",
    "    print(\"\\t Recall score: \", recall_score(y_true, y_predict))\n",
    "    print(\"\\t F1 score : \", f1_score(y_true, y_predict))\n",
    "\n",
    "    for i in range(len(y_predict)):\n",
    "        print(\"\\t True value:\",y_true[i] ,\"Prediction:\",y_predict[i])\n",
    "\n",
    "printMetrics(\"Logistic Regression :\",y_pred_log, y_test)\n",
    "printMetrics(\"K-Nearest Neighbor :\",y_pred_nn, y_test)\n",
    "printMetrics(\"Random Forest :\",y_pred_for, y_test)\n",
    "printMetrics(\"Gradient Bosting :\",y_pred_grad, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93a0d3d",
   "metadata": {},
   "source": [
    "#### Comments:\n",
    "For the moment, the number of messages given for the text are too low to have a good interpretation, so we will see it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b323a370",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd283b",
   "metadata": {},
   "source": [
    "This project was very usefull because it helps me to learn our to clean and made a text or a data understandible for a cumputer science task. Then I build models like in the others project to detect spam and ham email. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
