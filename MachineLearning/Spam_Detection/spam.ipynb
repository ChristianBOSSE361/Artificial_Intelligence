{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c044f5",
   "metadata": {},
   "source": [
    "# Project on Spam Detection\n",
    "\n",
    "- Date: Not marked - August 5 2025 \n",
    "\n",
    "- Data: The data used in this project come from  SpamAssasin Public Corpus. we can find them by this link: https://spamassassin.apache.org/old/publiccorpus/ \n",
    "\n",
    "- Description: This project aims to predict, regarding an email, if it's a spam or a ham."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c270349",
   "metadata": {},
   "source": [
    "# Cleaning of data\n",
    "\n",
    "Firstly we will try to take the data and transform it to a type we can use.\n",
    "Look deeper how chatgpt did it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e01e3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/christian/ProjetsPerso/IA/MachineLearning/Spam_Detection/data/20030228_easy_ham_2\n",
      "Chargés 6552 messages (2399 spam, 4153 ham)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from email import policy\n",
    "from email.parser import BytesParser\n",
    "\n",
    "def load_messages(root_folder=\"publiccorpus\"):\n",
    "    paths = glob.glob(f\"{root_folder}/**/*\", recursive=True)\n",
    "    texts, labels = [], []\n",
    "\n",
    "    print(paths[0])\n",
    "    for path in paths:\n",
    "        # Ignorer les dossiers\n",
    "        try:\n",
    "            with open(path, 'rb') as f:\n",
    "                msg = BytesParser(policy=policy.default).parse(f)\n",
    "        except IsADirectoryError:\n",
    "            continue\n",
    "\n",
    "        # Extraction du body (plain ou html)\n",
    "        body = msg.get_body(preferencelist=('plain', 'html'))\n",
    "        raw_text = \"\"\n",
    "        if body:\n",
    "            try:\n",
    "                # méthode standard (peut lever LookupError)\n",
    "                raw_text = body.get_content()\n",
    "            except LookupError:\n",
    "                # fallback : décoder le payload manuellement\n",
    "                payload = body.get_payload(decode=True) or b\"\"\n",
    "                try:\n",
    "                    raw_text = payload.decode('utf-8', errors='replace')\n",
    "                except (UnicodeDecodeError, LookupError):\n",
    "                    raw_text = payload.decode('latin-1', errors='replace')\n",
    "\n",
    "        texts.append(raw_text)\n",
    "        labels.append(0 if \"ham\" in path.lower() else 1)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    texts, labels = load_messages(\"/home/christian/ProjetsPerso/IA/MachineLearning/Spam_Detection/data\")\n",
    "    print(f\"Chargés {len(texts)} messages ({sum(labels)} spam, {len(labels)-sum(labels)} ham)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10990833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Quoting Niall O Broin <niall@linux.ie>:\\n\\n> I\\'m installing warm standby disks on a number of boxes. These disks will be\\n> the same size (sometimes bigger) than the main disk. The idea is that every\\n> night I\\'ll rsync the partitions on the main disk to the standby disk so\\n> that\\n> in the case of disaster, the first port of call, before the tapes, is the\\n> standby disk. (We did consider running Linux md RAID on the disks but RAID\\n> gives you no protection against slips of the finger)\\n\\nDo I get beaten round the head for saying \"floppy\"?\\nAssuming the machines are networked, let each one send a copy of its kernel to\\nthe others.  If the drives are open-the-box-and-switch-cables, then you can\\nstart dd\\'ing a floppy before you start.  If the drives are in drawers, then this\\nmight slow you down by all of 60 seconds.\\n\\nAlternatively, you could use netboot.  No, I\\'m serious.  Set the boot sequence\\nto first hard disk then network.  Do NOT make any partition on the standby\\nactive.  Have a look at the etherboot package.  One of the things it contains is\\na pascal-ish language for writing boot menus.  You can write a one-liner that\\nbasically says \"boot /dev/hda1\" (or whatever, there\\'s example code).  IIRC, the\\nresulting \"bootable image\" is a whopping 4K.  The downside is you\\'ll need a\\nbootp and tftp server somewhere....\\n\\nhth,\\nRonan.\\n\\n\\n-- \\nIrish Linux Users\\' Group: ilug@linux.ie\\nhttp://www.linux.ie/mailman/listinfo/ilug for (un)subscription information.\\nList maintainer: listmaster@linux.ie\\n\\n\\n', '\\nOn Tuesday, August 6, 2002, at 10:19  AM, Justin Mason wrote:\\n\\n>\\n> \"Craig R.Hughes\" said:\\n>\\n>> Excellent.  After that, I\\'d suggest we work on stabilizing (ie\\n>> no new features) for a bit, and getting 2.40 released.  I think\\n>> we\\'re in reasonably good shape now with 2.40 (plus or minus some\\n>> build issues occasionally creeping in), and Razor2 is starting\\n>> to get a little less flakey (minus one or two server issues in\\n>> the last few days).\\n>\\n> Yeah, I agree.  Although note: I\\'d like to see the proposal to use\\n> Received IPs in the AWL, get into 2.40 too.  That\\'s being actively\\n> exploited right now.\\n\\nYeah, AWL IPs is a good thing for 2.40\\n\\n> Razor2 is definitely getting solid, and getting some good hits too!\\n\\nWhen it works, yeah.  But it\\'s so flakey I have to turn it off \\nso that my mail can be guaranteed to actually get through in a \\ndecent timeframe.\\n\\nC\\n\\n\\n']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(texts[0:2])\n",
    "print(type(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0141e40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/christian/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3923ef",
   "metadata": {},
   "source": [
    "In this phase, we will do what we call preprocessing the text.\n",
    "\n",
    "It consists into tranform all characters in the text in a lowercase, then delete all special characters(:,@,!,?,...), all email adresses and keep only letters and numbers. Then we transform the text in different tokens and delete the stopwords(very used word with no real sens like \"the\",\"of\", \"is\", \"a\", \"but\",...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61ad01e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_nltk(text):\n",
    "    tokens=[]# For the text's tokens\n",
    "    stop_words = set(stopwords.words('english')) # all the stopword in english(eg:\"the\",\"is\",\"of\"...) because not very useful for our need\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    text = re.sub(r'\\S+@\\S+', ' ', text)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'[^A-Za-z0-9\\s]', ' ', text)\n",
    "    \n",
    "    #Conversion en minuscule\n",
    "    text=text.lower()\n",
    "    \n",
    "    #Suppression of pontuation\n",
    "    text = ''.join([char for char in text if char not in string.punctuation])\n",
    "\n",
    "    #Tokenization\n",
    "    tokens=word_tokenize(text)\n",
    "\n",
    "    #Suppresion of stopword\n",
    "    tokens=[w for w in tokens if w not in stop_words]\n",
    "\n",
    "    #Stemming (reduction of the number of token by gathering different variant of the same word or token)\n",
    "    tokens=[stemmer.stem(w) for w in tokens]\n",
    "\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772ddc2c",
   "metadata": {},
   "source": [
    "We will transform the text in a numeric vetor that could be used in our models. This transformation give a score we call TD-IDF which gave more importance to less used words in a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1df3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christian/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer=TfidfVectorizer(preprocessor=preprocess_text_nltk,\n",
    "                           tokenizer=lambda txt: txt.split())\n",
    "X=vectorizer.fit_transform(texts)\n",
    "y=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d38f664f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (6552, 46416)\n",
      "['0' '00' '000' ... 'zzzzason' 'zzzzcc' 'zzzzteana']\n",
      "Matrix:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Vocab size: 46416\n",
      "Some features:\n",
      " ['quot', 'niall', 'broin', 'instal', 'warm', 'standbi', 'disk', 'number', 'box', 'size', 'sometim', 'bigger', 'main', 'idea', 'everi', 'night', 'rsync', 'partit', 'case', 'disast']\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape:\", X.shape)\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(\"Matrix:\\n\",X.toarray())\n",
    "print(\"Vocab size:\", len(vectorizer.vocabulary_))\n",
    "print(\"Some features:\\n\", list(vectorizer.vocabulary_.keys())[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a371f215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e79eaa8",
   "metadata": {},
   "source": [
    "texts_raw = [\n",
    "    \"Hello @moimemeéé, this is a test email! Offer inside.\",\n",
    "    \"URGENT: You have won $1000. Click here!!!\",\n",
    "    \"Bonjour, ceci est un email légitime.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    preprocessor=preprocess_text_nltk,\n",
    "    tokenizer=lambda txt: txt.split(),\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "# 1 Debug preprocess\n",
    "for txt in texts_raw:\n",
    "    print(\"->\", preprocess_text_nltk(txt))\n",
    "\n",
    "# 2 Fit transform\n",
    "X_test = vectorizer.fit_transform(texts_raw)\n",
    "print(\"Shape:\", X_test.shape)\n",
    "print(\"Vocab:\", vectorizer.get_feature_names_out())\n",
    "print(\"Matrix:\\n\", X_test.toarray())\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(type(texts_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6f5274",
   "metadata": {},
   "source": [
    "# Building of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b473a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d26a76fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression :\n",
      "\t Accuracy score:  0.9557589626239512\n",
      "\t Precision score:  0.9709821428571429\n",
      "\t Recall score:  0.90625\n",
      "\t F1 score :  0.9375\n",
      "K-Nearest Neighbor :\n",
      "\t Accuracy score:  0.5316552250190694\n",
      "\t Precision score:  0.43703007518796994\n",
      "\t Recall score:  0.96875\n",
      "\t F1 score :  0.6023316062176166\n",
      "Random Forest :\n",
      "\t Accuracy score:  0.973302822273074\n",
      "\t Precision score:  0.9764453961456103\n",
      "\t Recall score:  0.95\n",
      "\t F1 score :  0.9630411826821542\n",
      "Gradient Bosting :\n",
      "\t Accuracy score:  0.9687261632341724\n",
      "\t Precision score:  0.9720430107526882\n",
      "\t Recall score:  0.9416666666666667\n",
      "\t F1 score :  0.9566137566137566\n"
     ]
    }
   ],
   "source": [
    "#Splitting the data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.8,\n",
    "                                               random_state=42,#keeping the same result\n",
    "                                               stratify=y)\n",
    "\n",
    "#Building of the models and fitting\n",
    "reg_log =LogisticRegression().fit(X_train,y_train)\n",
    "reg_for =RandomForestClassifier().fit(X_train,y_train)\n",
    "reg_grad=GradientBoostingClassifier().fit(X_train,y_train)\n",
    "reg_nn  =KNeighborsClassifier(n_neighbors=5).fit(X_train,y_train)\n",
    "\n",
    "#Prediction\n",
    "y_pred_log =reg_log.predict(X_test) \n",
    "y_pred_for =reg_for.predict(X_test) \n",
    "y_pred_grad=reg_grad.predict(X_test)\n",
    "y_pred_nn  =reg_nn.predict(X_test)\n",
    "\n",
    "#Evaluation\n",
    "def printMetrics(method_title, y_predict, y_true):\n",
    "    print(method_title) \n",
    "    print(\"\\t Accuracy score: \", accuracy_score(y_true, y_predict))\n",
    "    print(\"\\t Precision score: \", precision_score(y_true, y_predict))\n",
    "    print(\"\\t Recall score: \", recall_score(y_true, y_predict))\n",
    "    print(\"\\t F1 score : \", f1_score(y_true, y_predict))\n",
    "\n",
    "\n",
    "printMetrics(\"Logistic Regression :\",y_pred_log, y_test)\n",
    "printMetrics(\"K-Nearest Neighbor :\",y_pred_nn, y_test)\n",
    "printMetrics(\"Random Forest :\",y_pred_for, y_test)\n",
    "printMetrics(\"Gradient Bosting :\",y_pred_grad, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7c45f",
   "metadata": {},
   "source": [
    "We can notice that the logistic regression, the random forest and the Gradient bosting models are very good because their score are very high. However, the K-neirest neighbor model is not very appropriate for this task even if he predict correctly the good tweet and have the best score for that task ( highest score for the recall score)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba45826",
   "metadata": {},
   "source": [
    "# Test\n",
    "\n",
    "Here we will give to our models some real spam and ham messages recived and see what will be the result. (didn t find messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c133cc50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b323a370",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fd283b",
   "metadata": {},
   "source": [
    "This project was very usefull because it helps me to learn our to clean and made a text or a data understandible for a cumputer science task. Then I build models like in the others project to detect spam and ham email. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
