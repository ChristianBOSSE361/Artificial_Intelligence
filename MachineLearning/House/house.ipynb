{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68578c87",
   "metadata": {},
   "source": [
    "# Project: Prediction of house's prices\n",
    "\n",
    "- Date: July 7 2025\n",
    "\n",
    "- Data: The data used in that project came from an American census of 1990. We could find them in the modul datasets from sklearn, then we use the function fetch_california_housing to download the data.\n",
    "\n",
    "- Description: The goal of this project is to build some models able to predict the house's prices. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4c3baa",
   "metadata": {},
   "source": [
    "## Downloading of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "863319a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Data===\n",
      "        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  \n",
      "0        -122.23  \n",
      "1        -122.22  \n",
      "2        -122.24  \n",
      "3        -122.25  \n",
      "4        -122.25  \n",
      "...          ...  \n",
      "20635    -121.09  \n",
      "20636    -121.21  \n",
      "20637    -121.22  \n",
      "20638    -121.32  \n",
      "20639    -121.24  \n",
      "\n",
      "[20640 rows x 8 columns]\n",
      "===Target===\n",
      " 0        4.526\n",
      "1        3.585\n",
      "2        3.521\n",
      "3        3.413\n",
      "4        3.422\n",
      "         ...  \n",
      "20635    0.781\n",
      "20636    0.771\n",
      "20637    0.923\n",
      "20638    0.847\n",
      "20639    0.894\n",
      "Name: MedHouseVal, Length: 20640, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# on récupère le dataset California Housing\n",
    "dataset = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# DataFrame of the features\n",
    "X = dataset.data\n",
    "\n",
    "# Prices\n",
    "y = dataset.target\n",
    "\n",
    "# Display of the data\n",
    "print(\"===Data===\\n\",X)\n",
    "print(\"===Target===\\n\",y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc730e",
   "metadata": {},
   "source": [
    "We can notice that the prices are to low for houses, that normal because the price where divided by $100 000. So 4.5 means actually $450 000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4f6ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data':        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  \n",
      "0        -122.23  \n",
      "1        -122.22  \n",
      "2        -122.24  \n",
      "3        -122.25  \n",
      "4        -122.25  \n",
      "...          ...  \n",
      "20635    -121.09  \n",
      "20636    -121.21  \n",
      "20637    -121.22  \n",
      "20638    -121.32  \n",
      "20639    -121.24  \n",
      "\n",
      "[20640 rows x 8 columns], 'target': 0        4.526\n",
      "1        3.585\n",
      "2        3.521\n",
      "3        3.413\n",
      "4        3.422\n",
      "         ...  \n",
      "20635    0.781\n",
      "20636    0.771\n",
      "20637    0.923\n",
      "20638    0.847\n",
      "20639    0.894\n",
      "Name: MedHouseVal, Length: 20640, dtype: float64, 'frame':        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  MedHouseVal  \n",
      "0        -122.23        4.526  \n",
      "1        -122.22        3.585  \n",
      "2        -122.24        3.521  \n",
      "3        -122.25        3.413  \n",
      "4        -122.25        3.422  \n",
      "...          ...          ...  \n",
      "20635    -121.09        0.781  \n",
      "20636    -121.21        0.771  \n",
      "20637    -121.22        0.923  \n",
      "20638    -121.32        0.847  \n",
      "20639    -121.24        0.894  \n",
      "\n",
      "[20640 rows x 9 columns], 'target_names': ['MedHouseVal'], 'feature_names': ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude'], 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 20640\\n\\n:Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n:Attribute Information:\\n    - MedInc        median income in block group\\n    - HouseAge      median house age in block group\\n    - AveRooms      average number of rooms per household\\n    - AveBedrms     average number of bedrooms per household\\n    - Population    block group population\\n    - AveOccup      average number of household members\\n    - Latitude      block group latitude\\n    - Longitude     block group longitude\\n\\n:Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. rubric:: References\\n\\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n  Statistics and Probability Letters, 33 (1997) 291-297\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cacd5",
   "metadata": {},
   "source": [
    "## Importing the different modules.\n",
    "It is important to notice that there is a big difference between a regression and a classification.\n",
    "- A Regression: for predict numbers, real value like temperature, prices,...\n",
    "- A classification : for predict category and label like type of the flower, the disease,cat or dog,...\n",
    "\n",
    "So here, we will use regressors only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e552a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split,KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from numpy import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732a63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting of the data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26583698",
   "metadata": {},
   "source": [
    "# Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee92b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation of Models\n",
    "reg_log=LinearRegression()\n",
    "reg_tree=DecisionTreeRegressor()\n",
    "reg_nn=KNeighborsRegressor()\n",
    "reg_svr=SVR(kernel='rbf')\n",
    "\n",
    "#Training the models\n",
    "reg_log.fit(X_train,y_train)\n",
    "reg_tree.fit(X_train,y_train)\n",
    "reg_nn.fit(X_train,y_train)\n",
    "reg_svr.fit(X_train,y_train)\n",
    "\n",
    "#Predictions\n",
    "y_pred_log  = reg_log.predict(X_test)\n",
    "y_pred_tree = reg_tree.predict(X_test)\n",
    "y_pred_nn   = reg_nn.predict(X_test)\n",
    "y_pred_svr  = reg_svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00cc2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: \n",
      "\tRoot MSE       : 0.70364246988449\n",
      "\tR² score  : 0.6207378258250453\n",
      "Logistic regression: \n",
      "\tRoot MSE       : 0.707299774222593\n",
      "\tR² score  : 0.6167850172834648\n",
      "KNearest Neighbor: \n",
      "\tRoot MSE       : 1.0571715536327777\n",
      "\tR² score  : 0.1438962904581068\n",
      "SVM (kernel=rbf): \n",
      "\tRoot MSE       : 1.1524100957741938\n",
      "\tR² score  : -0.017301184608337294\n"
     ]
    }
   ],
   "source": [
    "def printMetrics(method_title, y_predict, y_true):\n",
    "    print(method_title) \n",
    "    print(\"\\tRoot MSE       :\", sqrt(mean_squared_error(y_true, y_predict)))\n",
    "    print(\"\\tR² score  :\", r2_score(y_true, y_predict))\n",
    "\n",
    "printMetrics(\"Decision Tree: \", y_pred_tree, y_test)\n",
    "printMetrics(\"Logistic regression: \",y_pred_log, y_test)\n",
    "printMetrics(\"KNearest Neighbor: \",y_pred_nn, y_test)\n",
    "printMetrics(\"SVR (kernel=rbf): \",y_pred_svr, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab8ee9",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "We can see here that the different models are not really good. Indeed,\n",
    "\n",
    "- The SVR model (whith the default parameters) :\n",
    "\n",
    "    Have one of the biggest error rate among the models chosen in this project and his 'r2 score' is negatif, that's means that it is worst than a model which just predict the average. His rate errors is around 1.10 that means that if the price of the house in the dataset were divided by 100,000 , we have an average difference of $110,000 between the prediction and the real value of the house, that is too much. That model is obviously not good enough with that parameters. Maybe we have to change the value of some parameters( as C, gamma,...) to have a better model. And according to ChatGPT, the SVM is not the first choice when we want to do regression like here.\n",
    "\n",
    "- The K-nearest neighbor(with default neigbors equals 5):\n",
    "    \n",
    "    Have also a big error rate. We have around 1.0 that means we will have a difference of $100.000 from the real value of the price, which is not acceptable. Also we have a 'r2 score' very close to 0 because it's around 0.1, we can say that this model is almost useless like the fisrt one it is just a little bit better than just taking the average.\n",
    "\n",
    "- The Logistic regressor:\n",
    "\n",
    "    Have a high rate error rate even if it is lower than the first 2 models. We will have with this model a difference of 70,000 from the real prices, and this is still not acceptable. However, the 'r2 score' is a bit close to 1, this model with predict almost corretly the deviation in our data. \n",
    "\n",
    "- The Decison Tree regressor:\n",
    "\n",
    "    Still have a high a error rate like the logistic regressor model. We will have a deviation of almost 70,000 from the real prices. Concerning the 'r2 score', it is amoung the best we have here like the previous one, it will be able to predict nearly correctly the deviation in our data.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61a89d",
   "metadata": {},
   "source": [
    "We are going to test a new model based on many decision tree (because it was one of the best model among those tested): The random forest.\n",
    "\n",
    "Let's see what we can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644cf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest :\n",
      "\tRoot MSE       : 0.493505261144027\n",
      "\tR² score  : 0.8134397927603711\n"
     ]
    }
   ],
   "source": [
    "#We try another model here : RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "reg_for=RandomForestRegressor().fit(X_train,y_train)\n",
    "y_pred_for=reg_for.predict(X_test)\n",
    "printMetrics(\"Random Forest(default number of estimators) :\",y_pred_for, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a3527",
   "metadata": {},
   "source": [
    "The Random forest model (with the default parameters) take more time but is the best among all models created for the moment. The 'r2_score' is closer to 1 and the root of MSE is lower. We will have a difference of almost $49,000 from the real price, which is a little bit good.  \n",
    "\n",
    "Let'see if we can involve the score by changing the parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab78518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_n_estimators  mean_test_score  std_test_score\n",
      "0                  10         0.778720        0.005599\n",
      "1                  50         0.795469        0.002391\n",
      "2                 100         0.799775        0.003571\n",
      "3                 200         0.799646        0.003398\n",
      "Best Random Forest between estimator=[10,50,100,200] :\n",
      "\tRoot MSE       : 0.4948874852170567\n",
      "\tR² score  : 0.8123932826134437\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "param_grid = {'n_estimators': [10, 50, 100, 200]}\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# All results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display\n",
    "print(results[['param_n_estimators', 'mean_test_score', 'std_test_score']])\n",
    "\n",
    "# We keep the best model\n",
    "best_Rdfor=grid_search.best_estimator_\n",
    "\n",
    "#Estimation\n",
    "y_pred_best_Rdfor =best_Rdfor.predict(X_test)\n",
    "printMetrics(\"Best Random Forest between estimator=[10,50,100,200] :\", y_pred_best_Rdfor , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a7199",
   "metadata": {},
   "source": [
    "We can notice that the score increases with number of parameters.\n",
    "\n",
    "However, the difference ( regarding the score ) between 100 estimators and 200 estimators is very small, It seems better to keep 100 estimators instead of 200 to have less running time. We notice also that thereis no a significant difference between the best model here and the default model( default parameter) chosen by the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c52a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try the last model with different parameter to see what will be the result : Done\n",
    "# Do also the cross validation : done\n",
    "# Next project, let see if I could work on unsupervised learning to change a litte bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dd379",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Now we want to use cross validation to re-evaluate our models with the cross_val_score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf7da27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.553031114027957\n",
      "Decision Tree : 0.3478390630692032\n",
      "K-Nearest Neighbors : 0.002334523135833111\n",
      "SVR : -0.1101188223291046\n",
      "Random Forest : 0.6532313714333096\n"
     ]
    }
   ],
   "source": [
    "#use stratifyKfold after.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "#We put the models and their scores in dictionaries.\n",
    "models={\"Logistic Regression :\":reg_log, \"Decision Tree :\":reg_tree, \"K-Nearest Neighbors :\": reg_nn,\"SVR :\": reg_svr, \"Random Forest :\": reg_for}\n",
    "scores={\"Logistic Regression :\": 0, \"Decision Tree :\": 0, \"K-Nearest Neighbors :\": 0,\"SVR :\": reg_svr, \"Random Forest :\": 0}\n",
    "\n",
    "#We fill the scores\n",
    "for key in models:\n",
    "    scores[key]=cross_val_score(models[key],X,y,cv=5, scoring=\"r2\")\n",
    "\n",
    "#We display\n",
    "for key in models:\n",
    "    print(key, np.mean(scores[key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc88a1",
   "metadata": {},
   "source": [
    "We can notice when we split more the dataset, the models have some problems to correctly predict prices. The only models whith their scores closer to 1 than to 0 are: the Logistic regression and the Random Forest.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Finally we can say that maybe the models used in that project are not very suitable for the regression (unsupervised learning) or the problem came from the dataset itself.\n",
    "\n",
    "According to ChatGPT the 3 best models for a regression are :\n",
    "- Gradient Bosting\n",
    "- Random Forest Regressor (we use it and it was our best model amoung those used)\n",
    "- Lasso Regressor\n",
    "\n",
    "We'll be using them in another project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
