{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68578c87",
   "metadata": {},
   "source": [
    "## Project on prediction of house's prices\n",
    "\n",
    "Date: July 7 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "863319a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  \n",
      "0        -122.23  \n",
      "1        -122.22  \n",
      "2        -122.24  \n",
      "3        -122.25  \n",
      "4        -122.25  \n",
      "...          ...  \n",
      "20635    -121.09  \n",
      "20636    -121.21  \n",
      "20637    -121.22  \n",
      "20638    -121.32  \n",
      "20639    -121.24  \n",
      "\n",
      "[20640 rows x 8 columns] 0        4.526\n",
      "1        3.585\n",
      "2        3.521\n",
      "3        3.413\n",
      "4        3.422\n",
      "         ...  \n",
      "20635    0.781\n",
      "20636    0.771\n",
      "20637    0.923\n",
      "20638    0.847\n",
      "20639    0.894\n",
      "Name: MedHouseVal, Length: 20640, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# on récupère le dataset California Housing\n",
    "dataset = fetch_california_housing(as_frame=True)\n",
    "\n",
    "# DataFrame of the features\n",
    "X = dataset.data\n",
    "\n",
    "# Prices\n",
    "y = dataset.target\n",
    "\n",
    "# Display of the data\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe19db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data':        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  \n",
      "0        -122.23  \n",
      "1        -122.22  \n",
      "2        -122.24  \n",
      "3        -122.25  \n",
      "4        -122.25  \n",
      "...          ...  \n",
      "20635    -121.09  \n",
      "20636    -121.21  \n",
      "20637    -121.22  \n",
      "20638    -121.32  \n",
      "20639    -121.24  \n",
      "\n",
      "[20640 rows x 8 columns], 'target': 0        4.526\n",
      "1        3.585\n",
      "2        3.521\n",
      "3        3.413\n",
      "4        3.422\n",
      "         ...  \n",
      "20635    0.781\n",
      "20636    0.771\n",
      "20637    0.923\n",
      "20638    0.847\n",
      "20639    0.894\n",
      "Name: MedHouseVal, Length: 20640, dtype: float64, 'frame':        MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
      "0      8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
      "1      8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
      "2      7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
      "3      5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
      "4      3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
      "...       ...       ...       ...        ...         ...       ...       ...   \n",
      "20635  1.5603      25.0  5.045455   1.133333       845.0  2.560606     39.48   \n",
      "20636  2.5568      18.0  6.114035   1.315789       356.0  3.122807     39.49   \n",
      "20637  1.7000      17.0  5.205543   1.120092      1007.0  2.325635     39.43   \n",
      "20638  1.8672      18.0  5.329513   1.171920       741.0  2.123209     39.43   \n",
      "20639  2.3886      16.0  5.254717   1.162264      1387.0  2.616981     39.37   \n",
      "\n",
      "       Longitude  MedHouseVal  \n",
      "0        -122.23        4.526  \n",
      "1        -122.22        3.585  \n",
      "2        -122.24        3.521  \n",
      "3        -122.25        3.413  \n",
      "4        -122.25        3.422  \n",
      "...          ...          ...  \n",
      "20635    -121.09        0.781  \n",
      "20636    -121.21        0.771  \n",
      "20637    -121.22        0.923  \n",
      "20638    -121.32        0.847  \n",
      "20639    -121.24        0.894  \n",
      "\n",
      "[20640 rows x 9 columns], 'target_names': ['MedHouseVal'], 'feature_names': ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude'], 'DESCR': '.. _california_housing_dataset:\\n\\nCalifornia Housing dataset\\n--------------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 20640\\n\\n:Number of Attributes: 8 numeric, predictive attributes and the target\\n\\n:Attribute Information:\\n    - MedInc        median income in block group\\n    - HouseAge      median house age in block group\\n    - AveRooms      average number of rooms per household\\n    - AveBedrms     average number of bedrooms per household\\n    - Population    block group population\\n    - AveOccup      average number of household members\\n    - Latitude      block group latitude\\n    - Longitude     block group longitude\\n\\n:Missing Attribute Values: None\\n\\nThis dataset was obtained from the StatLib repository.\\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\\n\\nThe target variable is the median house value for California districts,\\nexpressed in hundreds of thousands of dollars ($100,000).\\n\\nThis dataset was derived from the 1990 U.S. census, using one row per census\\nblock group. A block group is the smallest geographical unit for which the U.S.\\nCensus Bureau publishes sample data (a block group typically has a population\\nof 600 to 3,000 people).\\n\\nA household is a group of people residing within a home. Since the average\\nnumber of rooms and bedrooms in this dataset are provided per household, these\\ncolumns may take surprisingly large values for block groups with few households\\nand many empty houses, such as vacation resorts.\\n\\nIt can be downloaded/loaded using the\\n:func:`sklearn.datasets.fetch_california_housing` function.\\n\\n.. rubric:: References\\n\\n- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\\n  Statistics and Probability Letters, 33 (1997) 291-297\\n'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cacd5",
   "metadata": {},
   "source": [
    "# Importing the different modules.\n",
    "It is important to notice that there is a big difference between a regression and a classification.\n",
    "- A Regression: for predict numbers, real value like temperature, prices,...\n",
    "- A classification : for predict category and label like type of the flower, the disease,cat or dog,...\n",
    "\n",
    "So here, we will use regressors only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e552a46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split,KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from numpy import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7161d8bd",
   "metadata": {},
   "source": [
    "# Slipting of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "732a63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26583698",
   "metadata": {},
   "source": [
    "# Train and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee92b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialisation of Models\n",
    "reg_log=LinearRegression()\n",
    "reg_tree=DecisionTreeRegressor()\n",
    "reg_nn=KNeighborsRegressor()\n",
    "reg_svr=SVR(kernel='rbf')\n",
    "\n",
    "#Training the models\n",
    "reg_log.fit(X_train,y_train)\n",
    "reg_tree.fit(X_train,y_train)\n",
    "reg_nn.fit(X_train,y_train)\n",
    "reg_svr.fit(X_train,y_train)\n",
    "\n",
    "#Predictions\n",
    "y_pred_log  = reg_log.predict(X_test)\n",
    "y_pred_tree = reg_tree.predict(X_test)\n",
    "y_pred_nn   = reg_nn.predict(X_test)\n",
    "y_pred_svr  = reg_svr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d00cc2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree: \n",
      "\tRoot MSE       : 0.7199303892646153\n",
      "\tR² score  : 0.6026845722975606\n",
      "Logistic regression: \n",
      "\tRoot MSE       : 0.7114541957456256\n",
      "\tR² score  : 0.6119851862050933\n",
      "KNearest Neighbor: \n",
      "\tRoot MSE       : 1.0578801955133923\n",
      "\tR² score  : 0.14211832640095323\n",
      "SVM (kernel=rbf): \n",
      "\tRoot MSE       : 1.1519609451705821\n",
      "\tR² score  : -0.017255221428728484\n"
     ]
    }
   ],
   "source": [
    "def printMetrics(method_title, y_predict, y_true):\n",
    "    print(method_title) \n",
    "    print(\"\\tRoot MSE       :\", sqrt(mean_squared_error(y_true, y_predict)))\n",
    "    print(\"\\tR² score  :\", r2_score(y_true, y_predict))\n",
    "\n",
    "printMetrics(\"Decision Tree: \", y_pred_tree, y_test)\n",
    "printMetrics(\"Logistic regression: \",y_pred_log, y_test)\n",
    "printMetrics(\"KNearest Neighbor: \",y_pred_nn, y_test)\n",
    "printMetrics(\"SVM (kernel=rbf): \",y_pred_svr, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ab8ee9",
   "metadata": {},
   "source": [
    "# Interpretation\n",
    "We can see here that the different models are not really good. Indeed,\n",
    "\n",
    "- The SVM model (whith the default parameters) :\n",
    "\n",
    "    Have one of the biggest rate errors among the models chosen in this project and his 'r2 score' is negatif, that's means that it is worst than a model which just predict the average. His rate errors is around 1.18 that means that if the price of the house in the dataset were divided by 1000, we have an average difference of 1,100Euro between the prediction and the real value of the house, that is too much. That model is obviouly not goog enough with that parameters. Maybe we have to change the value of some parameters( as C, gamma,...) to have a better model. And according to ChatGPT, the SVM are not the first choice when we want to do regression like here.\n",
    "\n",
    "- The K-nearest neighbor:\n",
    "    \n",
    "    Have also a big rate errors\n",
    "\n",
    "- The Logistic regressor:\n",
    "\n",
    "\n",
    "\n",
    "- The Decison Tree regressor:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c61a89d",
   "metadata": {},
   "source": [
    "We are going to test a new model based on many decisiontree ( because it was  one of the best model among the tested one ): The random forest.\n",
    "\n",
    "Let's see what we can have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644cf1d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest :\n",
      "\tRoot MSE       : 0.5068249440642525\n",
      "\tR² score  : 0.8030888180761876\n"
     ]
    }
   ],
   "source": [
    "#We try another model here : RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "reg_for=RandomForestRegressor().fit(X_train,y_train)\n",
    "y_pred_for=reg_for.predict(X_test)\n",
    "printMetrics(\"Random Forest :\",y_pred_for, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35a3527",
   "metadata": {},
   "source": [
    "The Random forest model (with the default parameters) take more time but is the best among all models created for the moment. The r2_score is closer to 1 and the root of MSE is lower.\n",
    "\n",
    "Let'see if we can involve the score by changing the parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab78518b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   param_n_estimators  mean_test_score  std_test_score\n",
      "0                  10         0.781206        0.011244\n",
      "1                  50         0.801789        0.007480\n",
      "2                 100         0.803017        0.006909\n",
      "3                 200         0.803622        0.007405\n",
      "Best Random Forest between estimator=[10,50,100,200] :\n",
      "\tRoot MSE       : 0.5032002053830396\n",
      "\tR² score  : 0.8058953068340755\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "param_grid = {'n_estimators': [10, 50, 100, 200]}\n",
    "grid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# All results\n",
    "results = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display\n",
    "print(results[['param_n_estimators', 'mean_test_score', 'std_test_score']])\n",
    "\n",
    "# We keep the best model\n",
    "best_Rdfor=grid_search.best_estimator_\n",
    "\n",
    "#Estimation\n",
    "y_pred_best_Rdfor =best_Rdfor.predict(X_test)\n",
    "printMetrics(\"Best Random Forest between estimator=[10,50,100,200] :\", y_pred_best_Rdfor , y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614a7199",
   "metadata": {},
   "source": [
    "We can notice that the score increases with number of parameters.\n",
    "\n",
    "However, the difference ( regarding the score ) between 100 estimators and 200 estimators is very small, It seems better to keep 100 estimators instead of 200 to have less running time. We notice also that there no a significant difference between the best model here and the default model( default parameter) chosen by the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c52a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#try the last model with different parameter to see what will be the result : Done\n",
    "# Do also the cross validation :\n",
    "# Next project, let see if I could work on unsupervised learning to change a litte bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516dd379",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "Now we want to use cross validation to re-evaluate our models with the cross_val_score function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7da27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.553031114027957\n",
      "Decision Tree : 0.3280474968935586\n",
      "K-Nearest Neighbors : 0.002334523135833111\n",
      "SVR : -0.1101188223291046\n",
      "Random Forest : 0.6486290375266777\n"
     ]
    }
   ],
   "source": [
    "#use stratifyKfold after.\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "#We put the models and their scores in dictionaries.\n",
    "models={\"Logistic Regression :\":reg_log, \"Decision Tree :\":reg_tree, \"K-Nearest Neighbors :\": reg_nn,\"SVR :\": reg_svr, \"Random Forest :\": reg_for}\n",
    "scores={\"Logistic Regression :\": 0, \"Decision Tree :\": 0, \"K-Nearest Neighbors :\": 0,\"SVR :\": reg_svr, \"Random Forest :\": 0}\n",
    "\n",
    "#We fill the scores\n",
    "for key in models:\n",
    "    scores[key]=cross_val_score(models[key],X,y,cv=5, scoring=\"r2\")\n",
    "\n",
    "#We display\n",
    "for key in models:\n",
    "    print(key, np.mean(scores[key]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc88a1",
   "metadata": {},
   "source": [
    "We can notice when we split more the dataset, the models have some problems to correctly predict prices. The only models whith their scores closer to 1 than to 0 are: the Logistic regression and the Random Forest.\n",
    "\n",
    "Maybe the models used in that project are not very suitable for the regression (unsupervised learning) or the problem came from the dataset itself.\n",
    "\n",
    "According to ChatGPT the 3 best models for a regression are :\n",
    "- Gradient Bosting\n",
    "- Random Forest Regressor\n",
    "- Lasso Regressor\n",
    "\n",
    "We'll be using them in another project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4413cfcd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
